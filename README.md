# GPT-26M: A 26 Million Parameter GPT Model

A lightweight GPT implementation trained on the TinyStories dataset with ~26 million parameters. This project includes custom attention mechanisms, RoPE (Rotary Position Embeddings), and optional Mixture of Experts (MoE) support.

## ğŸ“ Project Structure

```
gpt_26m/
â”œâ”€â”€ src/                    # Core model implementation
â”‚   â”œâ”€â”€ gpt.py             # Main GPT model architecture
â”‚   â”œâ”€â”€ attention.py       # Multi-head attention implementation
â”‚   â”œâ”€â”€ moe.py             # Mixture of Experts layer
â”‚   â”œâ”€â”€ rope.py            # Rotary Position Embeddings
â”‚   â”œâ”€â”€ config.py          # Model configuration
â”‚   â”œâ”€â”€ tokenizer.py       # Tokenizer setup
â”‚   â””â”€â”€ dataloader.py      # Dataset loading and preprocessing
â”œâ”€â”€ scripts/               # Training and inference scripts
â”‚   â”œâ”€â”€ train.py          # Training script with wandb logging
â”‚   â””â”€â”€ test_generate.py  # Text generation script
â”œâ”€â”€ notebooks/            # Jupyter notebooks for experiments
â”œâ”€â”€ assets/              # Model weights and tokenizer files
â”‚   â”œâ”€â”€ 3hr_gpt_model.pth
â”‚   â””â”€â”€ tinystories_tokenizer1.json
â”œâ”€â”€ checkpoints/         # Training checkpoints (gitignored)
â”œâ”€â”€ data/               # Dataset cache (gitignored)
â””â”€â”€ requirements.txt    # Python dependencies
```

## ğŸš€ Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/AkshithAI/gpt_26m.git
cd gpt_26m
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Training

Train the model from scratch:
```bash
python scripts/train.py
```

Training features:
- Mixed precision training (FP16)
- Gradient clipping
- Cosine learning rate schedule with warmup
- Early stopping
- WandB integration for experiment tracking
- Automatic checkpointing and artifact logging

### Text Generation

Generate text using a trained model:
```bash
python scripts/test_generate.py
```

## ğŸ—ï¸ Model Architecture

- **Parameters**: ~26 million
- **Embedding Dimension**: Configurable (see `src/config.py`)
- **Attention Heads**: Multi-head self-attention
- **Position Encoding**: RoPE (Rotary Position Embeddings)
- **Context Length**: Configurable
- **Optional**: Mixture of Experts (MoE) layers

## ğŸ“Š Training Configuration

Key hyperparameters (configurable in `src/config.py`):
- Learning rate with cosine scheduling
- Warmup steps: 5000
- Optimizer: AdamW with weight decay
- Mixed precision training (FP16)
- Gradient clipping: max norm 1.0

## ğŸ“ˆ Monitoring

The training script logs metrics to Weights & Biases:
- Training loss (per batch and per epoch)
- Validation loss
- Learning rate
- Gradient norms
- Model checkpoints as artifacts

## ğŸ¯ Dataset

Trained on [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset - a collection of short stories generated for training small language models.

## ğŸ“ License

MIT License

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub.
