{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12977680,"sourceType":"datasetVersion","datasetId":8214102},{"sourceId":13008974,"sourceType":"datasetVersion","datasetId":8235933}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import AdamW\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:31:36.331278Z","iopub.execute_input":"2025-09-18T08:31:36.331471Z","iopub.status.idle":"2025-09-18T08:31:40.481691Z","shell.execute_reply.started":"2025-09-18T08:31:36.331446Z","shell.execute_reply":"2025-09-18T08:31:40.480911Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load TinyStories\nfrom datasets import load_dataset\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntrain_data = dataset[\"train\"]\nval_data = dataset[\"validation\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T17:34:15.434665Z","iopub.execute_input":"2025-09-10T17:34:15.435125Z","iopub.status.idle":"2025-09-10T17:34:19.934321Z","shell.execute_reply.started":"2025-09-10T17:34:15.435076Z","shell.execute_reply":"2025-09-10T17:34:19.933653Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.models import BPE\nfrom tokenizers.processors import TemplateProcessing\ndef get_training_corpus():\n    for i in range(0, len(dataset), 1000):\n        yield train_data[i : i + 1000][\"text\"]\n\ngenerator = get_training_corpus()\ntokenizer = Tokenizer(BPE())\ntokenizer.pre_tokenizer = Whitespace()\nspecial_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\", \"[MASK]\"]\ntrainer =  BpeTrainer(\n    vocab_size = 30000,\n    show_progress = True,\n    special_tokens = special_tokens,\n)\ntokenizer.train_from_iterator(generator,trainer)\n\n# Add BOS/EOS tokens for GPT training\ntokenizer.post_processor = TemplateProcessing(\n    single=\"[BOS] $A [EOS]\",\n    special_tokens=[(\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")), \n                   (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\"))]\n)\ntokenizer.save(\"tinystories_tokenizer1.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom transformers import AutoTokenizer,PreTrainedTokenizerFast\ntokenizer = Tokenizer.from_file(\"/kaggle/input/tokenizer/tinystories_tokenizer1.json\")\ntokenizer = PreTrainedTokenizerFast(tokenizer_object = tokenizer)\ntokenizer.pad_token_id = 0\ntokenizer.unk_token_id = 1\ntokenizer.bos_token_id = 2\ntokenizer.eos_token_id = 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:32:06.428135Z","iopub.execute_input":"2025-09-18T08:32:06.428398Z","iopub.status.idle":"2025-09-18T08:32:10.143080Z","shell.execute_reply.started":"2025-09-18T08:32:06.428377Z","shell.execute_reply":"2025-09-18T08:32:10.142501Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pprint\npprint.pprint(train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T17:34:25.701626Z","iopub.execute_input":"2025-09-10T17:34:25.702427Z","iopub.status.idle":"2025-09-10T17:34:25.706976Z","shell.execute_reply.started":"2025-09-10T17:34:25.702397Z","shell.execute_reply":"2025-09-10T17:34:25.706152Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['text'],\n    num_rows: 2119719\n})\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class TinyStoryDataset(Dataset):\n    def __init__(self,data):\n        super().__init__()\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self,idx):\n        encoded = tokenizer(self.data[idx],\n                    padding = \"max_length\",\n                    truncation = True,\n                    max_length = config.max_seq_len,\n                    return_tensors = \"pt\",\n                    return_attention_mask = True \n                   )\n        return {\n            \"input_ids\" : encoded[\"input_ids\"].squeeze(0),\n            \"attention_mask\" : encoded[\"attention_mask\"].squeeze(0)\n        }\n\ntrain_data = TinyStoryDataset(train_data[\"text\"])\nval_data = TinyStoryDataset(val_data[\"text\"])\n\ntrain_loader = DataLoader(train_data,batch_size = 64,shuffle = True,pin_memory = True)\nval_loader = DataLoader(val_data,batch_size = 64,shuffle = False,pin_memory = True)\n\nprint(len(train_loader))\nprint(len(val_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T17:34:27.624835Z","iopub.execute_input":"2025-09-10T17:34:27.625401Z","iopub.status.idle":"2025-09-10T17:34:32.235660Z","shell.execute_reply.started":"2025-09-10T17:34:27.625377Z","shell.execute_reply":"2025-09-10T17:34:32.234866Z"}},"outputs":[{"name":"stdout","text":"33121\n344\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class FeedForwardNet(nn.Module):\n    def __init__(self,d_model,dropout = 0.2):\n        super(FeedForwardNet,self).__init__()\n        self.d_model = d_model\n        self.w_1 = nn.Linear(d_model,4 * d_model)\n        self.w_2 = nn.Linear(4 * d_model , d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self,x):\n        x = self.w_1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        final = self.w_2(x)\n        return final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:33:15.086743Z","iopub.execute_input":"2025-09-18T08:33:15.087172Z","iopub.status.idle":"2025-09-18T08:33:15.092253Z","shell.execute_reply.started":"2025-09-18T08:33:15.087151Z","shell.execute_reply":"2025-09-18T08:33:15.091557Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PE_Vec(nn.Module):\n    def __init__(self,d_model,max_len = 512):\n        super(PE_Vec,self).__init__()\n        pos = torch.arange(0,max_len,dtype = torch.float).unsqueeze(1)\n        pe = torch.zeros(max_len,d_model)\n        div_term = torch.exp(torch.arange(0,d_model,2) * - math.log(10000)/d_model)\n        pe[:,0::2] = torch.sin(pos * div_term)\n        pe[:,1::2] = torch.cos(pos * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self,x):\n        return self.add_with_offset(x,0)\n\n    def add_with_offset(self,x,offset):\n        batch_size,seq_len,_ = x.shape\n        positions = torch.arange(offset, offset + seq_len, device=x.device).unsqueeze(0)\n        pos_enc = self.pe[:,positions,:].to(x.device)\n        return x + pos_enc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:33:17.588397Z","iopub.execute_input":"2025-09-18T08:33:17.588716Z","iopub.status.idle":"2025-09-18T08:33:17.594897Z","shell.execute_reply.started":"2025-09-18T08:33:17.588693Z","shell.execute_reply":"2025-09-18T08:33:17.594283Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MultiHeadAttentionVec(nn.Module):\n    def __init__(self,d_model,heads = 8,mask = True):\n        super(MultiHeadAttentionVec,self).__init__()\n        assert d_model % heads == 0\n        self.d_model = d_model\n        self.heads = heads\n        self.d_k = self.d_model // self.heads\n        self.mask = mask\n\n        self.w_q = nn.Linear(d_model,d_model,bias = False)\n        self.w_k = nn.Linear(d_model,d_model,bias = False)\n        self.w_v = nn.Linear(d_model,d_model,bias = False)\n\n        self.w_o= nn.Linear(d_model,d_model,bias = False)\n\n    def forward(self,x,att_mask,past_key_value = None,use_cache = False): \n        if len(x.shape) == 4 and x.size(0) == 1:\n            x = x.squeeze(0)\n        batch_size,seq_len,_ = x.shape\n        \n        Q = self.w_q(x).view(batch_size,seq_len,self.heads,self.d_k).transpose(1,2)\n        K = self.w_k(x).view(batch_size,seq_len,self.heads,self.d_k).transpose(1,2)\n        V = self.w_v(x).view(batch_size,seq_len,self.heads,self.d_k).transpose(1,2)\n        if past_key_value is not None:\n            past_k,past_v = past_key_value\n            K = torch.cat([past_k,K],dim = 2)\n            V = torch.cat([past_v,V],dim = 2)\n        # full_seq_len = K.shape[2]\n        present_key_value = (K, V) if use_cache else None\n        scores = torch.matmul(Q,K.transpose(-2,-1)) / math.sqrt(self.d_k) #(B,H,S,S)\n        pad_mask = att_mask.unsqueeze(1).unsqueeze(1)  # Shape: [B, 1, 1, full_seq_len]\n        scores = scores.masked_fill(pad_mask == 0, float('-65504.0'))\n        if self.mask:\n            full_seq_len = scores.shape[2]\n            causal_mask = torch.tril(torch.ones(seq_len,full_seq_len,dtype=torch.bool,device = x.device))\n            scores = scores.masked_fill(causal_mask == 0,float('-65504.0'))\n        # if att_mask is not None:\n        #     # Extend attention mask to cover the full sequence length\n        #     if att_mask.shape[-1] != full_seq_len:\n        #         # This handles the case where we have cached keys\n        #         extended_mask = torch.ones(batch_size, full_seq_len, device=x.device, dtype=att_mask.dtype)\n        #         extended_mask[:, -att_mask.shape[-1]:] = att_mask\n        #         att_mask = extended_mask\n            \n        #     att_mask = att_mask.unsqueeze(1).unsqueeze(1)  # Shape: [B, 1, 1, full_seq_len]\n        #     scores = scores.masked_fill(att_mask == 0, float('-65504.0'))\n        att_scores = F.softmax(scores,dim = -1)\n        final = torch.matmul(att_scores,V).transpose(1,2).contiguous().view(batch_size,seq_len,self.heads * self.d_k)\n        out = self.w_o(final)\n        return out,present_key_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:33:25.972111Z","iopub.execute_input":"2025-09-18T08:33:25.972793Z","iopub.status.idle":"2025-09-18T08:33:25.987109Z","shell.execute_reply.started":"2025-09-18T08:33:25.972760Z","shell.execute_reply":"2025-09-18T08:33:25.986290Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class TransformerDecoderBLK(nn.Module):\n    def __init__(self,d_model,heads,use_cache = False):\n        super(TransformerDecoderBLK,self).__init__()\n        self.d_model = d_model\n        self.heads = heads\n        self.use_cache = use_cache\n        self.attention = MultiHeadAttentionVec(d_model,heads,mask = True)\n        self.ffn = FeedForwardNet(d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self,x,att_mask,prev_key_value = None,use_cache = False):\n        attn_out,present_key_value = self.attention(\n            self.norm1(x),att_mask,prev_key_value,use_cache\n        )\n        x = x + attn_out\n        x = x + self.ffn(self.norm2(x))\n        return x,present_key_value\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:33:29.356193Z","iopub.execute_input":"2025-09-18T08:33:29.356914Z","iopub.status.idle":"2025-09-18T08:33:29.364187Z","shell.execute_reply.started":"2025-09-18T08:33:29.356884Z","shell.execute_reply":"2025-09-18T08:33:29.363494Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class GPT(nn.Module):\n    def __init__(self,d_model,heads,depth,max_len,vocab_size):\n        super(GPT,self).__init__()\n        self.d_model = d_model\n        self.heads = heads\n        self.max_len = max_len\n        self.embeddings = nn.Embedding(vocab_size,d_model)\n\n        self.layers = nn.ModuleList([TransformerDecoderBLK(d_model,heads) for _ in range(depth)])\n        self.pos_encoding = PE_Vec(d_model,max_len)\n        self.out = nn.Linear(d_model,vocab_size)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self,x,att_mask,past_key_values = None,use_cache = False):\n        batch_size,seq_len = x.shape\n        x = self.dropout(self.embeddings(x))\n        print(\"Akshith is gayy./...\")\n        \n        if past_key_values is not None:\n            # Use your original logic\n            position_offset = past_key_values[0][0].size(2)\n            x = self.pos_encoding.add_with_offset(x, position_offset)\n        else:\n            x = self.pos_encoding(x)\n            \n        present_key_values = [] if use_cache else None\n        \n        for i,layer in enumerate(self.layers):\n            # Fix the indexing logic - keep it simple like your original\n            prev_key_value_layer = past_key_values[i] if past_key_values is not None else None\n            if use_cache:\n                x, present_key_value = layer(x,att_mask,prev_key_value_layer,use_cache)\n                present_key_values.append(present_key_value)\n            else:\n                x,_ = layer(x,att_mask,prev_key_value_layer,use_cache)\n        if use_cache:\n            return self.out(x), present_key_values\n        return self.out(x), None\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:33:32.772054Z","iopub.execute_input":"2025-09-18T08:33:32.772897Z","iopub.status.idle":"2025-09-18T08:33:32.779721Z","shell.execute_reply.started":"2025-09-18T08:33:32.772862Z","shell.execute_reply":"2025-09-18T08:33:32.779110Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class GPTConfig:\n    def __init__(self):\n        # Model architecture\n        self.vocab_size = tokenizer.vocab_size\n        self.n_layer = 6          # Your 6-layer model\n        self.n_head = 8           # Number of attention heads\n        self.n_embd = 512         # Embedding dimension\n        self.max_seq_len = 512    # Maximum sequence length\n        \n        # Training\n        self.dropout = 0.1\n        self.learning_rate = 3e-4\n        self.batch_size = 64\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nconfig = GPTConfig()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:33:36.372301Z","iopub.execute_input":"2025-09-18T08:33:36.372977Z","iopub.status.idle":"2025-09-18T08:33:36.434552Z","shell.execute_reply.started":"2025-09-18T08:33:36.372950Z","shell.execute_reply":"2025-09-18T08:33:36.433945Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nmax_epochs = 50\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = GPT(config.n_embd,config.n_head,config.n_layer,config.max_seq_len,tokenizer.vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\noptimizer = AdamW(model.parameters(),lr = config.learning_rate, betas=(0.9, 0.98), eps=1e-8, weight_decay=0.01)\nnum_warmup_steps = 3000  \n#num_training_steps = len(train_loader) * max_epochs\n# scheduler = get_cosine_schedule_with_warmup(\n#     optimizer,\n#     num_warmup_steps=num_warmup_steps,\n#     num_training_steps=num_training_steps\n# )\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(count_parameters(model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:35:19.038117Z","iopub.execute_input":"2025-09-18T08:35:19.038863Z","iopub.status.idle":"2025-09-18T08:35:19.308518Z","shell.execute_reply.started":"2025-09-18T08:35:19.038838Z","shell.execute_reply":"2025-09-18T08:35:19.307914Z"}},"outputs":[{"name":"stdout","text":"26234866\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def validate(model,val_loader,criterion,device):\n    model.eval()\n    avg_val_losses = 0\n    for x in tqdm(val_loader):\n        with torch.no_grad():\n            input_ids = x[\"input_ids\"].to(device)\n            attn_mask = x[\"attention_mask\"].to(device)\n    \n            inputs = input_ids[:,:-1]\n            targets = input_ids[:,1:]\n\n            with torch.amp.autocast(device_type = config.device,dtype = torch.float16):\n                logits,_ = model(inputs,attn_mask[:,:-1],use_cache = True)\n                logits_view = logits.contiguous().view(-1,config.vocab_size)\n                targets_view = targets.contiguous().view(-1)\n                loss = criterion(logits_view,targets_view)\n\n            avg_val_losses += loss.item()\n    return avg_val_losses / len(val_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.amp import autocast,GradScaler\nimport time\nfrom tqdm import tqdm\n\nlosses = []\nglobal_step = 0\nbest_val_loss = float('inf')\npatience = 5  \npatience_counter = 0\nscaler = GradScaler()  \n\n# Track metrics\ntrain_losses = []\nval_losses = []\ntraining_start_time = time.time()\n\nfor epoch in range(max_epochs):\n    print(f\"Epoch {epoch + 1}/{max_epochs}\")\n    model.train()\n    epoch_losses = []\n    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n    for step,x in enumerate(progress_bar):\n        optimizer.zero_grad()\n        input_ids = x[\"input_ids\"].to(config.device)\n        attn_mask = x[\"attention_mask\"].to(config.device)\n\n        inputs = input_ids[:,:-1]\n        targets = input_ids[:,1:]\n        with autocast(device_type = config.device,dtype = torch.float16):\n            logits,_ = model(inputs,attn_mask[:,:-1],use_cache = True)\n            logits_view = logits.contiguous().view(-1,config.vocab_size)\n            targets_view = targets.contiguous().view(-1)\n            loss = criterion(logits_view,targets_view)\n        \n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(),2.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        loss_value = loss.item()\n        losses.append(loss_value)\n        epoch_losses.append(loss_value)\n\n        progress_bar.set_postfix({\"loss\": f\"{loss_value:.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n\n    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n    train_losses.append(avg_epoch_loss)\n\n    val_loss = validate(model,val_loader,criterion,config.device)\n    val_losses.append(val_loss)\n    print(f\"Validation Loss: {val_loss:.4f}\")\n\n    # Save checkpoint for this epoch\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': Transformer.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'scaler_state_dict': scaler.state_dict(),\n        'train_loss': avg_epoch_loss,\n        'val_loss': val_loss,\n    }, f\"transformer_checkpoint_epoch_{epoch}.pth\")\n\n    # Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        patience_counter = 0\n        # Save best model\n        torch.save(Transformer.state_dict(), \"best_transformer_model.pth\")\n        print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"Early stopping triggered\")\n            break\n\n    # Print Epoch Summary\n    print(f\"  Epoch {epoch + 1} completed - Average Loss: {avg_epoch_loss:.4f}\")\n    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n    print(f\"  Time elapsed: {(time.time() - training_start_time)/60:.2f} minutes\")\n    print(\"-\" * 50)\n    \n# Plot training history\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\nplt.savefig('training_history.png')\nplt.show()\n\nprint(f\"Training completed in {(time.time() - training_start_time)/60:.2f} minutes\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T17:37:52.703800Z","iopub.execute_input":"2025-09-10T17:37:52.704348Z","iopub.status.idle":"2025-09-10T17:38:01.667650Z","shell.execute_reply.started":"2025-09-10T17:37:52.704323Z","shell.execute_reply":"2025-09-10T17:38:01.666456Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   0%|          | 7/33121 [00:08<11:43:19,  1.27s/it, loss=1.9435, lr=0.000001]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_365/1159861544.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"path = torch.load(\"/kaggle/input/2hr-tr/transformer_checkpoint_epoch_2hr.pth\")[\"model_state_dict\"]\nmodel.load_state_dict(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:59:08.508104Z","iopub.execute_input":"2025-09-18T08:59:08.508384Z","iopub.status.idle":"2025-09-18T08:59:08.833726Z","shell.execute_reply.started":"2025-09-18T08:59:08.508364Z","shell.execute_reply":"2025-09-18T08:59:08.833074Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import time\n@torch.inference_mode()\ndef generate(model,device,tokenizer,seed_txt,max_len = 500):\n    temp = 0.6\n    model.eval()\n    seed_tokens = torch.tensor([tokenizer.bos_token_id] + tokenizer.encode(seed_txt),device = device).unsqueeze(0)\n    attn_mask = torch.ones_like(seed_tokens).to(device)\n    generated = []\n    for _ in range(max_len):\n        logits,_ = model(seed_tokens,attn_mask,use_cache = False)\n        if len(logits.shape) == 4:\n            next_token_logits = logits[0, 0, -1, :]  \n        elif len(logits.shape) == 3:\n            next_token_logits = logits[0, -1, :]  \n        else:\n            raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n        \n        probs = F.softmax(next_token_logits / temp,dim = -1)\n        vocab_size = len(tokenizer)\n        idx = torch.multinomial(probs,num_samples = 1)\n        idx_scalar = torch.clamp(idx, 0, vocab_size - 1).item()\n        new_token = torch.tensor([[idx_scalar]], device=device)\n        seed_tokens = torch.cat([seed_tokens,new_token],dim = -1)\n        generated.append(idx_scalar)\n        attn_mask = torch.ones_like(seed_tokens)\n        if idx_scalar == tokenizer.eos_token_id:\n            break\n        \n    return tokenizer.decode(generated)  \nseed_txt = \"a young boy\"\nst = time.time()\ntxt = generate(model,device,tokenizer,seed_txt)\nend = time.time()\nprint(end-st)\nprint(txt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T08:59:22.322688Z","iopub.execute_input":"2025-09-18T08:59:22.322960Z","iopub.status.idle":"2025-09-18T08:59:23.227077Z","shell.execute_reply.started":"2025-09-18T08:59:22.322941Z","shell.execute_reply":"2025-09-18T08:59:23.226460Z"}},"outputs":[{"name":"stdout","text":"0.8885955810546875\nwas very excited to go to the park . He had never been to the park before and was always so happy . He ran around , looking for something fun to do . Suddenly , he saw a lot of kids playing and he wanted to join them , but he was a bit scared . He looked around and saw a group of kids playing together . He asked them if he wanted to join them and they said yes . The boy was so excited he ran to join them . But when he got there , he realized he was feeling scared and was going to be okay . The kids were so scared and so they ran away . They ran and ran until they were gone . The boy was so ashamed that he didn ' t get hurt or scared anymore . He had to go to the hospital and he was very sad . He had been so silly and he had to go to the hospital . [EOS]\n","output_type":"stream"}],"execution_count":15}]}