{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-18T08:31:36.331471Z",
     "iopub.status.busy": "2025-09-18T08:31:36.331278Z",
     "iopub.status.idle": "2025-09-18T08:31:40.481691Z",
     "shell.execute_reply": "2025-09-18T08:31:40.480911Z",
     "shell.execute_reply.started": "2025-09-18T08:31:36.331446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T17:34:15.435125Z",
     "iopub.status.busy": "2025-09-10T17:34:15.434665Z",
     "iopub.status.idle": "2025-09-10T17:34:19.934321Z",
     "shell.execute_reply": "2025-09-10T17:34:19.933653Z",
     "shell.execute_reply.started": "2025-09-10T17:34:15.435076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load TinyStories\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield train_data[i : i + 1000][\"text\"]\n",
    "\n",
    "generator = get_training_corpus()\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\", \"[MASK]\"]\n",
    "trainer =  BpeTrainer(\n",
    "    vocab_size = 30000,\n",
    "    show_progress = True,\n",
    "    special_tokens = special_tokens,\n",
    ")\n",
    "tokenizer.train_from_iterator(generator,trainer)\n",
    "\n",
    "# Add BOS/EOS tokens for GPT training\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    special_tokens=[(\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")), \n",
    "                   (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\"))]\n",
    ")\n",
    "tokenizer.save(\"tinystories_tokenizer1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:32:06.428398Z",
     "iopub.status.busy": "2025-09-18T08:32:06.428135Z",
     "iopub.status.idle": "2025-09-18T08:32:10.143080Z",
     "shell.execute_reply": "2025-09-18T08:32:10.142501Z",
     "shell.execute_reply.started": "2025-09-18T08:32:06.428377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import AutoTokenizer,PreTrainedTokenizerFast\n",
    "tokenizer = Tokenizer.from_file(\"/kaggle/input/tokenizer/tinystories_tokenizer1.json\")\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object = tokenizer)\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.unk_token_id = 1\n",
    "tokenizer.bos_token_id = 2\n",
    "tokenizer.eos_token_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T17:34:25.702427Z",
     "iopub.status.busy": "2025-09-10T17:34:25.701626Z",
     "iopub.status.idle": "2025-09-10T17:34:25.706976Z",
     "shell.execute_reply": "2025-09-10T17:34:25.706152Z",
     "shell.execute_reply.started": "2025-09-10T17:34:25.702397Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2119719\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T17:34:27.625401Z",
     "iopub.status.busy": "2025-09-10T17:34:27.624835Z",
     "iopub.status.idle": "2025-09-10T17:34:32.235660Z",
     "shell.execute_reply": "2025-09-10T17:34:32.234866Z",
     "shell.execute_reply.started": "2025-09-10T17:34:27.625377Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33121\n",
      "344\n"
     ]
    }
   ],
   "source": [
    "class TinyStoryDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        encoded = tokenizer(self.data[idx],\n",
    "                    padding = \"max_length\",\n",
    "                    truncation = True,\n",
    "                    max_length = config.max_seq_len,\n",
    "                    return_tensors = \"pt\",\n",
    "                    return_attention_mask = True \n",
    "                   )\n",
    "        return {\n",
    "            \"input_ids\" : encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\" : encoded[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "train_data = TinyStoryDataset(train_data[\"text\"])\n",
    "val_data = TinyStoryDataset(val_data[\"text\"])\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size = 64,shuffle = True,pin_memory = True)\n",
    "val_loader = DataLoader(val_data,batch_size = 64,shuffle = False,pin_memory = True)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:15.087172Z",
     "iopub.status.busy": "2025-09-18T08:33:15.086743Z",
     "iopub.status.idle": "2025-09-18T08:33:15.092253Z",
     "shell.execute_reply": "2025-09-18T08:33:15.091557Z",
     "shell.execute_reply.started": "2025-09-18T08:33:15.087151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self,d_model,dropout = 0.2):\n",
    "        super(FeedForwardNet,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.w_1 = nn.Linear(d_model,4 * d_model)\n",
    "        self.w_2 = nn.Linear(4 * d_model , d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        x = self.w_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        final = self.w_2(x)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:17.588716Z",
     "iopub.status.busy": "2025-09-18T08:33:17.588397Z",
     "iopub.status.idle": "2025-09-18T08:33:17.594897Z",
     "shell.execute_reply": "2025-09-18T08:33:17.594283Z",
     "shell.execute_reply.started": "2025-09-18T08:33:17.588693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PE_Vec(nn.Module):\n",
    "    def __init__(self,d_model,max_len = 512):\n",
    "        super(PE_Vec,self).__init__()\n",
    "        pos = torch.arange(0,max_len,dtype = torch.float).unsqueeze(1)\n",
    "        pe = torch.zeros(max_len,d_model)\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2) * - math.log(10000)/d_model)\n",
    "        pe[:,0::2] = torch.sin(pos * div_term)\n",
    "        pe[:,1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.add_with_offset(x,0)\n",
    "\n",
    "    def add_with_offset(self,x,offset):\n",
    "        batch_size,seq_len,_ = x.shape\n",
    "        positions = torch.arange(offset, offset + seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_enc = self.pe[:,positions,:].to(x.device)\n",
    "        return x + pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:25.972793Z",
     "iopub.status.busy": "2025-09-18T08:33:25.972111Z",
     "iopub.status.idle": "2025-09-18T08:33:25.987109Z",
     "shell.execute_reply": "2025-09-18T08:33:25.986290Z",
     "shell.execute_reply.started": "2025-09-18T08:33:25.972760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionVec(nn.Module):\n",
    "    def __init__(self,d_model,heads = 8,mask = True):\n",
    "        super(MultiHeadAttentionVec,self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = self.d_model // self.heads\n",
    "        self.mask = mask\n",
    "\n",
    "        self.w_q = nn.Linear(d_model,d_model,bias = False)\n",
    "        self.w_k = nn.Linear(d_model,d_model,bias = False)\n",
    "        self.w_v = nn.Linear(d_model,d_model,bias = False)\n",
    "\n",
    "        self.w_o= nn.Linear(d_model,d_model,bias = False)\n",
    "\n",
    "    def forward(self,x,att_mask,past_key_value = None,use_cache = False): \n",
    "        if len(x.shape) == 4 and x.size(0) == 1:\n",
    "            x = x.squeeze(0)\n",
    "        batch_size,seq_len,_ = x.shape\n",
    "        \n",
    "        Q = self.w_q(x).view(batch_size,seq_len,self.heads,self.d_k).transpose(1,2)\n",
    "        K = self.w_k(x).view(batch_size,seq_len,self.heads,self.d_k).transpose(1,2)\n",
    "        V = self.w_v(x).view(batch_size,seq_len,self.heads,self.d_k).transpose(1,2)\n",
    "        if past_key_value is not None:\n",
    "            past_k,past_v = past_key_value\n",
    "            K = torch.cat([past_k,K],dim = 2)\n",
    "            V = torch.cat([past_v,V],dim = 2)\n",
    "        # full_seq_len = K.shape[2]\n",
    "        present_key_value = (K, V) if use_cache else None\n",
    "        scores = torch.matmul(Q,K.transpose(-2,-1)) / math.sqrt(self.d_k) #(B,H,S,S)\n",
    "        pad_mask = att_mask.unsqueeze(1).unsqueeze(1)  # Shape: [B, 1, 1, full_seq_len]\n",
    "        scores = scores.masked_fill(pad_mask == 0, float('-65504.0'))\n",
    "        if self.mask:\n",
    "            full_seq_len = scores.shape[2]\n",
    "            causal_mask = torch.tril(torch.ones(seq_len,full_seq_len,dtype=torch.bool,device = x.device))\n",
    "            scores = scores.masked_fill(causal_mask == 0,float('-65504.0'))\n",
    "        att_scores = F.softmax(scores,dim = -1)\n",
    "        final = torch.matmul(att_scores,V).transpose(1,2).contiguous().view(batch_size,seq_len,self.heads * self.d_k)\n",
    "        out = self.w_o(final)\n",
    "        return out,present_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:29.356914Z",
     "iopub.status.busy": "2025-09-18T08:33:29.356193Z",
     "iopub.status.idle": "2025-09-18T08:33:29.364187Z",
     "shell.execute_reply": "2025-09-18T08:33:29.363494Z",
     "shell.execute_reply.started": "2025-09-18T08:33:29.356884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBLK(nn.Module):\n",
    "    def __init__(self,d_model,heads,use_cache = False):\n",
    "        super(TransformerDecoderBLK,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.use_cache = use_cache\n",
    "        self.attention = MultiHeadAttentionVec(d_model,heads,mask = True)\n",
    "        self.ffn = FeedForwardNet(d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,x,att_mask,prev_key_value = None,use_cache = False):\n",
    "        attn_out,present_key_value = self.attention(\n",
    "            self.norm1(x),att_mask,prev_key_value,use_cache\n",
    "        )\n",
    "        x = x + attn_out\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x,present_key_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:32.772897Z",
     "iopub.status.busy": "2025-09-18T08:33:32.772054Z",
     "iopub.status.idle": "2025-09-18T08:33:32.779721Z",
     "shell.execute_reply": "2025-09-18T08:33:32.779110Z",
     "shell.execute_reply.started": "2025-09-18T08:33:32.772862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self,d_model,heads,depth,max_len,vocab_size):\n",
    "        super(GPT,self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.max_len = max_len\n",
    "        self.embeddings = nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([TransformerDecoderBLK(d_model,heads) for _ in range(depth)])\n",
    "        self.pos_encoding = PE_Vec(d_model,max_len)\n",
    "        self.out = nn.Linear(d_model,vocab_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self,x,att_mask,past_key_values = None,use_cache = False):\n",
    "        batch_size,seq_len = x.shape\n",
    "        x = self.dropout(self.embeddings(x))\n",
    "        \n",
    "        if past_key_values is not None:\n",
    "            position_offset = past_key_values[0][0].size(2)\n",
    "            x = self.pos_encoding.add_with_offset(x, position_offset)\n",
    "        else:\n",
    "            x = self.pos_encoding(x)\n",
    "            \n",
    "        present_key_values = [] if use_cache else None\n",
    "        \n",
    "        for i,layer in enumerate(self.layers):\n",
    "            prev_key_value_layer = past_key_values[i] if past_key_values is not None else None\n",
    "            if use_cache:\n",
    "                x, present_key_value = layer(x,att_mask,prev_key_value_layer,use_cache)\n",
    "                present_key_values.append(present_key_value)\n",
    "            else:\n",
    "                x,_ = layer(x,att_mask,prev_key_value_layer,use_cache)\n",
    "        if use_cache:\n",
    "            return self.out(x), present_key_values\n",
    "        return self.out(x), None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:36.372977Z",
     "iopub.status.busy": "2025-09-18T08:33:36.372301Z",
     "iopub.status.idle": "2025-09-18T08:33:36.434552Z",
     "shell.execute_reply": "2025-09-18T08:33:36.433945Z",
     "shell.execute_reply.started": "2025-09-18T08:33:36.372950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    def __init__(self):\n",
    "        # Model architecture\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.n_layer = 6          # Your 6-layer model\n",
    "        self.n_head = 8           # Number of attention heads\n",
    "        self.n_embd = 512         # Embedding dimension\n",
    "        self.max_seq_len = 512    # Maximum sequence length\n",
    "        \n",
    "        # Training\n",
    "        self.dropout = 0.1\n",
    "        self.learning_rate = 3e-4\n",
    "        self.batch_size = 64\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:35:19.038863Z",
     "iopub.status.busy": "2025-09-18T08:35:19.038117Z",
     "iopub.status.idle": "2025-09-18T08:35:19.308518Z",
     "shell.execute_reply": "2025-09-18T08:35:19.307914Z",
     "shell.execute_reply.started": "2025-09-18T08:35:19.038838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26234866\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "max_epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT(config.n_embd,config.n_head,config.n_layer,config.max_seq_len,tokenizer.vocab_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "optimizer = AdamW(model.parameters(),lr = config.learning_rate, betas=(0.9, 0.98), eps=1e-8, weight_decay=0.01)\n",
    "num_warmup_steps = 3000  \n",
    "num_training_steps = len(train_loader) * max_epochs\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(model,val_loader,criterion,device):\n",
    "    model.eval()\n",
    "    avg_val_losses = 0\n",
    "    for x in tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "            input_ids = x[\"input_ids\"].to(device)\n",
    "            attn_mask = x[\"attention_mask\"].to(device)\n",
    "    \n",
    "            inputs = input_ids[:,:-1]\n",
    "            targets = input_ids[:,1:]\n",
    "\n",
    "            with torch.amp.autocast(device_type = config.device,dtype = torch.float16):\n",
    "                logits,_ = model(inputs,attn_mask[:,:-1],use_cache = True)\n",
    "                logits_view = logits.contiguous().view(-1,config.vocab_size)\n",
    "                targets_view = targets.contiguous().view(-1)\n",
    "                loss = criterion(logits_view,targets_view)\n",
    "\n",
    "            avg_val_losses += loss.item()\n",
    "    return avg_val_losses / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T17:37:52.704348Z",
     "iopub.status.busy": "2025-09-10T17:37:52.703800Z",
     "iopub.status.idle": "2025-09-10T17:38:01.667650Z",
     "shell.execute_reply": "2025-09-10T17:38:01.666456Z",
     "shell.execute_reply.started": "2025-09-10T17:37:52.704323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.amp import autocast,GradScaler\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "losses = []\n",
    "global_step = 0\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  \n",
    "patience_counter = 0\n",
    "scaler = GradScaler()  \n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    for step,x in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = x[\"input_ids\"].to(config.device)\n",
    "        attn_mask = x[\"attention_mask\"].to(config.device)\n",
    "\n",
    "        inputs = input_ids[:,:-1]\n",
    "        targets = input_ids[:,1:]\n",
    "        with autocast(device_type = config.device,dtype = torch.float16):\n",
    "            logits,_ = model(inputs,attn_mask[:,:-1],use_cache = True)\n",
    "            logits_view = logits.contiguous().view(-1,config.vocab_size)\n",
    "            targets_view = targets.contiguous().view(-1)\n",
    "            loss = criterion(logits_view,targets_view)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),2.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "        epoch_losses.append(loss_value)\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss_value:.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "\n",
    "    val_loss = validate(model,val_loader,criterion,config.device)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint for this epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "        'train_loss': avg_epoch_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, f\"transformer_checkpoint_epoch_{epoch}.pth\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), \"best_transformer_model.pth\")\n",
    "        print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # Print Epoch Summary\n",
    "    print(f\"  Epoch {epoch + 1} completed - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    print(f\"  Time elapsed: {(time.time() - training_start_time)/60:.2f} minutes\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training completed in {(time.time() - training_start_time)/60:.2f} minutes\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:59:08.508384Z",
     "iopub.status.busy": "2025-09-18T08:59:08.508104Z",
     "iopub.status.idle": "2025-09-18T08:59:08.833726Z",
     "shell.execute_reply": "2025-09-18T08:59:08.833074Z",
     "shell.execute_reply.started": "2025-09-18T08:59:08.508364Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = torch.load(\"/kaggle/input/2hr-tr/transformer_checkpoint_epoch_2hr.pth\")[\"model_state_dict\"]\n",
    "model.load_state_dict(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:59:22.322960Z",
     "iopub.status.busy": "2025-09-18T08:59:22.322688Z",
     "iopub.status.idle": "2025-09-18T08:59:23.227077Z",
     "shell.execute_reply": "2025-09-18T08:59:23.226460Z",
     "shell.execute_reply.started": "2025-09-18T08:59:22.322941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8885955810546875\n",
      "was very excited to go to the park . He had never been to the park before and was always so happy . He ran around , looking for something fun to do . Suddenly , he saw a lot of kids playing and he wanted to join them , but he was a bit scared . He looked around and saw a group of kids playing together . He asked them if he wanted to join them and they said yes . The boy was so excited he ran to join them . But when he got there , he realized he was feeling scared and was going to be okay . The kids were so scared and so they ran away . They ran and ran until they were gone . The boy was so ashamed that he didn ' t get hurt or scared anymore . He had to go to the hospital and he was very sad . He had been so silly and he had to go to the hospital . [EOS]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "@torch.inference_mode()\n",
    "def generate(model,device,tokenizer,seed_txt,max_len = 500):\n",
    "    temp = 0.6\n",
    "    model.eval()\n",
    "    seed_tokens = torch.tensor([tokenizer.bos_token_id] + tokenizer.encode(seed_txt),device = device).unsqueeze(0)\n",
    "    attn_mask = torch.ones_like(seed_tokens).to(device)\n",
    "    generated = []\n",
    "    for _ in range(max_len):\n",
    "        logits,_ = model(seed_tokens,attn_mask,use_cache = False)\n",
    "        if len(logits.shape) == 4:\n",
    "            next_token_logits = logits[0, 0, -1, :]  \n",
    "        elif len(logits.shape) == 3:\n",
    "            next_token_logits = logits[0, -1, :]  \n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected logits shape: {logits.shape}\")\n",
    "        \n",
    "        probs = F.softmax(next_token_logits / temp,dim = -1)\n",
    "        vocab_size = len(tokenizer)\n",
    "        idx = torch.multinomial(probs,num_samples = 1)\n",
    "        idx_scalar = torch.clamp(idx, 0, vocab_size - 1).item()\n",
    "        new_token = torch.tensor([[idx_scalar]], device=device)\n",
    "        seed_tokens = torch.cat([seed_tokens,new_token],dim = -1)\n",
    "        generated.append(idx_scalar)\n",
    "        attn_mask = torch.ones_like(seed_tokens)\n",
    "        if idx_scalar == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "    return tokenizer.decode(generated)  \n",
    "seed_txt = \"a young boy\"\n",
    "st = time.time()\n",
    "txt = generate(model,device,tokenizer,seed_txt)\n",
    "end = time.time()\n",
    "print(end-st)\n",
    "print(txt)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8214102,
     "sourceId": 12977680,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8235933,
     "sourceId": 13008974,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
